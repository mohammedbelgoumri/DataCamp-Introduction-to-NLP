{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Imports\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##########################################\n",
    "\n",
    "# Loading data\n",
    "\n",
    "# my_string\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
    "\n",
    "\n",
    "# scene_one\n",
    "with open(\"assets/grail.txt\") as file:\n",
    "    lines = file.readlines()[:40]\n",
    "    scene_one = ''.join(lines)\n",
    "\n",
    "# tweets:\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python', '#NLP is super fun! <3 #learning', 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# german_text\n",
    "german_text = \"Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\"\n",
    "\n",
    "# holy_grail\n",
    "with open(\"assets/grail.txt\") as file:\n",
    "    holy_grail = file.read()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'course', 'could', 'creeper', 'south', 'Wait', 'Pendragon', 'that', 'interested', 'Supposing', 'five', '--', 'master', 'court', 'breadth', \"'em\", \"'m\", 'swallow', 'pound', 'wants', 'knights', 'son', 'to', 'if', \"'d\", 'through', 'temperate', 'horse', 'plover', 'weight', 'then', 'sovereign', 'just', 'under', 'air-speed', 'A', 'clop', 'coconut', 'Pull', 'do', 'maintain', 'Britons', 'Are', 'land', 'Please', 'Halt', '#', 'its', 'point', 'swallows', 'line', 'SCENE', '...', '[', \"'\", '!', 'my', 'Whoa', 'goes', 'It', 'get', 'held', 'beat', 'So', 'at', 'Arthur', 'Where', 'Listen', 'bring', 'have', 'Camelot', 'carry', 'King', 'order', 'husk', 'What', 'speak', 'other', \"'ve\", 'be', 'he', 'two', 'grip', 'tell', 'from', 'kingdom', 'seek', 'Well', 'anyway', 'there', 'minute', 'is', '1', 'The', 'them', 'every', 'strand', 'carried', 'We', \"n't\", 'times', '.', 'together', 'must', 'trusty', 'use', 'all', 'strangers', 'right', 'question', 'may', 'feathers', 'non-migratory', 'SOLDIER', 'will', 'castle', 'Ridden', 'the', 'You', 'since', 'it', 'simple', 'bird', 'European', \"'re\", 'house', 'Will', 'are', 'servant', 'velocity', '2', 'snows', 'Found', 'ask', 'halves', 'second', 'this', 'covered', 'go', ',', 'bangin', 'martin', 'forty-three', 'they', 'an', 'and', 'back', 'one', 'ratios', 'on', 'climes', 'They', 'does', 'maybe', 'dorsal', 'Not', 'warmer', 'suggesting', 'Uther', 'That', 'your', 'matter', 'Patsy', 'ARTHUR', 'winter', 'in', 'No', 'Oh', 'of', 'length', 'mean', 'here', 'KING', 'defeator', 'England', 'you', 'a', 'me', '?', ':', 'Yes', \"'s\", 'but', 'guiding', ']', 'coconuts', 'yet', 'lord', 'our', 'fly', 'wings', 'Saxons', 'got', 'who', 'empty', 'not', 'by', 'grips', 'tropical', 'using', 'Court', 'ounce', 'agree', 'zone', 'where', 'carrying', 'with', 'search', 'why', 'needs', 'African', 'I', 'wind', 'Am', 'found', 'Mercea', 'join', 'yeah', 'ridden', 'sun', 'or', 'these', 'But', 'am', 'migrate', 'In', 'Who'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(r\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "580 588\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "print(hashtags)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['#nlp', '#python']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Write a pattern that matches both mentions (@) and hashtags\n",
    "pattern2 = r\"([#@]\\w+)\"\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
    "print(mentions_hashtags)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@datacamp', '#nlp', '#python']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capitalized words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
      "['Wann', 'Pizza', 'Und', 'Über']\n",
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}